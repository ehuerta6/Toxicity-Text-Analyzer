# ToxiGuard â€“ Detecta Comentarios TÃ³xicos

ToxiGuard es una aplicaciÃ³n web que analiza y clasifica comentarios en tiempo real, detectando contenido tÃ³xico y ofreciendo informaciÃ³n Ãºtil sobre el nivel de toxicidad. Ideal para moderadores, comunidades online o cualquier aplicaciÃ³n que quiera mantener un ambiente sano.

## ðŸŽ¯ Objetivo

Construir un MVP funcional que permita:

- Ingresar un comentario en la web
- Clasificarlo como TÃ³xico o No TÃ³xico
- Mostrar un score de toxicidad (0-100) y categorÃ­a bÃ¡sica
- Servir como base para futuras mejoras y funcionalidades avanzadas

## ðŸ›  Stack tecnolÃ³gico

- **Frontend:** React + TypeScript + TailwindCSS (Vite)
- **Backend:** FastAPI (Python) con endpoints REST
- **ML / NLP:** scikit-learn (TF-IDF + Logistic Regression), spaCy para preprocesamiento
- **Base de datos:** SQLite (opcional para MVP)
- **Deploy:** Frontend en Vercel/Netlify, Backend en Render

## ðŸ“‚ Estructura del proyecto

```
toxiguard/
â”œâ”€â”€ backend/          # FastAPI + modelo ML
â”œâ”€â”€ frontend/         # React + Vite + TypeScript
â”œâ”€â”€ models/           # Modelos entrenados y vectores TF-IDF
â”œâ”€â”€ data/             # Dataset de entrenamiento
â”œâ”€â”€ README.md         # Este archivo
â””â”€â”€ ROADMAP.md        # Plan detallado del proyecto
```

## ðŸš€ Inicio rÃ¡pido

1. Ver el [ROADMAP.md](./ROADMAP.md) para el plan completo del proyecto
2. Seguir las instrucciones en cada carpeta para configurar el entorno
3. Comenzar con la Fase 1: Setup inicial

## ðŸ“š DocumentaciÃ³n

- [ROADMAP.md](./ROADMAP.md) - Plan detallado del proyecto
- [backend/README.md](./backend/README.md) - ConfiguraciÃ³n del backend
- [frontend/README.md](./frontend/README.md) - ConfiguraciÃ³n del frontend
- [models/README.md](./models/README.md) - InformaciÃ³n sobre modelos ML
- [data/README.md](./data/README.md) - InformaciÃ³n sobre datasets

---

_Proyecto en desarrollo - Fase 1: Setup inicial_
