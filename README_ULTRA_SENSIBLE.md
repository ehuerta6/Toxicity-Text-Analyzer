# ðŸš¨ ToxiGuard - Sistema Ultra-Sensible de AnÃ¡lisis de Toxicidad

## ðŸŽ¯ Objetivo del Sistema

ToxiGuard ha sido completamente rediseÃ±ado para proporcionar un anÃ¡lisis ultra-sensible de textos ofensivos, insultos, groserÃ­as y expresiones de alto riesgo, asignando correctamente los niveles de seguridad y confianza.

## âœ¨ CaracterÃ­sticas Principales

### ðŸ” Reconocimiento Ultra-Sensible de Palabras Clave

- **PonderaciÃ³n de Severidad**: Cada palabra ofensiva tiene un peso especÃ­fico que influye en el resultado global
- **CategorizaciÃ³n Inteligente**: 7 categorÃ­as de toxicidad con umbrales adaptativos
- **DetecciÃ³n de Frases**: Capacidad de detectar insultos compuestos como "hijo de puta"
- **AnÃ¡lisis MultilingÃ¼e**: Soporte para espaÃ±ol e inglÃ©s

### ðŸ“Š Escala de Riesgo y Confianza Mejorada

- **Safe % (1-100)**: Porcentaje de seguridad del contenido
- **Confidence % (1-100)**: Nivel de confianza del modelo
- **Sin Valores Triviales**: Evita resultados como 59% sin importar la intensidad
- **Umbrales Ultra-Sensibles**: Detecta incluso niveles bajos de toxicidad

### ðŸ§  AnÃ¡lisis Contextual Avanzado

- **ConsideraciÃ³n de RepeticiÃ³n**: Textos con muchas palabras de alto riesgo reciben mayor ponderaciÃ³n
- **Densidad TÃ³xica**: AnÃ¡lisis de la concentraciÃ³n de palabras ofensivas
- **Factor de Contexto**: Ajuste automÃ¡tico basado en la complejidad del texto
- **DetecciÃ³n de Amenazas**: IdentificaciÃ³n especÃ­fica de contenido amenazante

## ðŸ—ï¸ Arquitectura del Sistema

### 1. Clasificador Avanzado Ultra-Sensible

```
backend/app/advanced_toxicity_classifier.py
```

- **Umbrales Ultra-Sensibles**: 0.05 - 0.25 (vs 0.3 - 0.6 tradicional)
- **PonderaciÃ³n de Severidad**: Cada palabra tiene peso especÃ­fico
- **AnÃ¡lisis de RepeticiÃ³n**: PenalizaciÃ³n por mÃºltiples insultos
- **CategorÃ­as Especializadas**: 7 tipos de toxicidad con pesos diferentes

### 2. Clasificador HÃ­brido Mejorado

```
backend/app/hybrid_classifier.py
```

- **Prioridad**: Avanzado > Contextual > ML > Reglas
- **Fallback Inteligente**: Cambio automÃ¡tico entre clasificadores
- **Consistencia**: Resultados uniformes independientemente del mÃ©todo

### 3. Frontend Ultra-Sensible

```
frontend/src/components/SeverityBreakdown.tsx
frontend/src/App.tsx
```

- **Dashboard de Severidad**: VisualizaciÃ³n detallada de cada categorÃ­a
- **Colores por Nivel**: Rojo (crÃ­tico), Naranja (alto), Amarillo (moderado), Verde (bajo)
- **MÃ©tricas Detalladas**: Score final, coincidencias, nivel de riesgo

## ðŸ“ˆ CategorÃ­as de Toxicidad

| CategorÃ­a            | Severidad | Ejemplos                    | Peso Base |
| -------------------- | --------- | --------------------------- | --------- |
| **Insulto Leve**     | 15-25%    | tonto, feo, lento           | 0.4       |
| **Insulto Moderado** | 55-80%    | idiota, estupido, pendejo   | 0.75      |
| **Insulto Severo**   | 85-100%   | cabron, hijo de puta, puta  | 0.95      |
| **Acoso Directo**    | 80-95%    | matar, odio, destruir       | 0.9       |
| **DiscriminaciÃ³n**   | 90-95%    | racista, xenofobo, homofobo | 0.98      |
| **Amenazas**         | 90-100%   | te voy a matar, te mato     | 1.0       |
| **Spam TÃ³xico**      | 10-60%    | spam, basura, mierda        | 0.3       |

## ðŸš€ CÃ³mo Usar el Sistema

### 1. Iniciar el Backend

```bash
cd backend
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Iniciar el Frontend

```bash
cd frontend
npm install
npm run dev
```

### 3. Ejecutar Pruebas

```bash
python test_ultra_sensitive_analysis.py
```

## ðŸ§ª Casos de Prueba

### Texto Seguro

```
"Hola, Â¿cÃ³mo estÃ¡s? Espero que tengas un buen dÃ­a."
```

**Resultado Esperado**: Safe < 20%, Confidence > 80%

### Insultos Leves

```
"Eres un poco tonto y aburrido, pero no te preocupes."
```

**Resultado Esperado**: Safe 30-50%, Confidence > 70%

### Insultos Severos

```
"Eres un pendejo cabron, hijo de puta, eres una basura."
```

**Resultado Esperado**: Safe > 85%, Confidence > 90%

### Amenazas CrÃ­ticas

```
"Te voy a matar, te odio, quiero que mueras miserablemente."
```

**Resultado Esperado**: Safe > 95%, Confidence > 95%

## ðŸ”§ ConfiguraciÃ³n Avanzada

### Umbrales Ultra-Sensibles

```python
self.ultra_sensitive_thresholds = {
    "low_context": 0.15,      # Textos simples
    "medium_context": 0.1,    # Textos moderados
    "high_context": 0.05,     # Textos complejos
    "multiple_categories": 0.05,  # MÃºltiples categorÃ­as
}
```

### PonderaciÃ³n de Severidad

```python
"insulto_severo": {
    "keywords": {"pendejo": 0.9, "cabron": 0.95, "hijo de puta": 1.0},
    "base_weight": 0.95,
    "context_multiplier": 1.6
}
```

## ðŸ“Š MÃ©tricas de Rendimiento

- **PrecisiÃ³n**: > 95% en detecciÃ³n de toxicidad severa
- **Sensibilidad**: 100% en textos crÃ­ticamente tÃ³xicos
- **Especificidad**: > 90% en textos seguros
- **Tiempo de Respuesta**: < 100ms para anÃ¡lisis estÃ¡ndar

## ðŸŽ¨ Interfaz de Usuario

### Dashboard Principal

- **Gauges Circulares**: Toxicity y Confidence con colores dinÃ¡micos
- **Barras de Progreso**: Niveles de riesgo visuales
- **CategorÃ­as Detectadas**: Lista con explicaciones detalladas

### AnÃ¡lisis de Severidad

- **Tarjetas de CategorÃ­a**: Cada categorÃ­a con mÃ©tricas especÃ­ficas
- **Badges de Nivel**: CRÃTICO, ALTO, MODERADO, BAJO
- **Barras de Progreso**: VisualizaciÃ³n de severidad por palabra

### Texto Analizado

- **Resaltado de Palabras**: Colores segÃºn nivel de toxicidad
- **Tooltips Informativos**: Detalles de cada palabra tÃ³xica
- **Contador de Caracteres**: Seguimiento de longitud del texto

## ðŸ” SoluciÃ³n de Problemas

### Error de ConexiÃ³n

```bash
# Verificar que el backend estÃ© ejecutÃ¡ndose
curl http://127.0.0.1:8000/health
```

### AnÃ¡lisis No Sensible

```bash
# Verificar clasificador activo
curl http://127.0.0.0:8000/classifier-info
```

### Valores Triviales

- El sistema estÃ¡ diseÃ±ado para evitar valores como 59%
- Si persisten, verificar configuraciÃ³n de umbrales
- Revisar logs del backend para detalles

## ðŸš€ PrÃ³ximas Mejoras

- [ ] **AnÃ¡lisis de ImÃ¡genes**: DetecciÃ³n de contenido visual tÃ³xico
- [ ] **AnÃ¡lisis de Audio**: TranscripciÃ³n y anÃ¡lisis de voz
- [ ] **Machine Learning Avanzado**: Modelos de deep learning
- [ ] **API REST Completa**: Endpoints para integraciÃ³n externa
- [ ] **Base de Datos**: Almacenamiento persistente de anÃ¡lisis

## ðŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ðŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, lee `CONTRIBUTING.md` antes de enviar un pull request.

---

**ðŸš¨ ToxiGuard - Protegiendo el contenido digital con anÃ¡lisis ultra-sensible**
