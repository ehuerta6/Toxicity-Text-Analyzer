#!/usr/bin/env python3
"""
ğŸ¯ DemostraciÃ³n del AnÃ¡lisis Contextual - ToxiGuard
Muestra las mejoras implementadas en el anÃ¡lisis de toxicidad
"""

from app.contextual_classifier import contextual_classifier
import time

def demo_contextual_analysis():
    """Demuestra el anÃ¡lisis contextual con ejemplos especÃ­ficos"""
    
    print("ğŸ¯ ToxiGuard - DemostraciÃ³n del AnÃ¡lisis Contextual")
    print("=" * 70)
    print("âœ… Mejoras implementadas:")
    print("   â€¢ AnÃ¡lisis contextual con embeddings")
    print("   â€¢ DetecciÃ³n de negaciones y contexto")
    print("   â€¢ AnÃ¡lisis por oraciones")
    print("   â€¢ Similitud semÃ¡ntica")
    print("   â€¢ Modelo ligero: all-MiniLM-L6-v2")
    print("=" * 70)
    
    # Casos de demostraciÃ³n
    demo_cases = [
        {
            "title": "ğŸ”´ Insulto Directo",
            "text": "Eres un idiota completo",
            "expected": "Alta toxicidad (insulto directo)"
        },
        {
            "title": "ğŸŸ¢ Insulto Negado (Contexto Positivo)",
            "text": "No eres tonto, eres muy inteligente",
            "expected": "Baja toxicidad (negaciÃ³n + contexto positivo)"
        },
        {
            "title": "ğŸ”´ Acoso Directo",
            "text": "Te voy a matar si no haces lo que digo",
            "expected": "Alta toxicidad (amenaza directa)"
        },
        {
            "title": "ğŸŸ¢ Acoso Negado",
            "text": "No te voy a hacer daÃ±o, solo quiero ayudarte",
            "expected": "Baja toxicidad (negaciÃ³n de violencia)"
        },
        {
            "title": "ğŸ”´ DiscriminaciÃ³n Directa",
            "text": "Los negros son inferiores a los blancos",
            "expected": "Alta toxicidad (discriminaciÃ³n)"
        },
        {
            "title": "ğŸŸ¢ DiscriminaciÃ³n Negada",
            "text": "Todos somos iguales, no discrimino a nadie",
            "expected": "Baja toxicidad (mensaje positivo)"
        },
        {
            "title": "ğŸŸ¡ Texto Neutral",
            "text": "El clima estÃ¡ muy agradable hoy",
            "expected": "Baja toxicidad (texto neutral)"
        },
        {
            "title": "ğŸŸ¡ Texto Mixto",
            "text": "Eres un idiota, pero no te voy a hacer daÃ±o",
            "expected": "Toxicidad moderada (insulto + negaciÃ³n)"
        }
    ]
    
    results = []
    
    for i, case in enumerate(demo_cases, 1):
        print(f"\nğŸ“ {i}. {case['title']}")
        print(f"Texto: '{case['text']}'")
        print(f"Esperado: {case['expected']}")
        
        start_time = time.time()
        result = contextual_classifier.analyze_text(case['text'])
        analysis_time = (time.time() - start_time) * 1000
        
        toxicity = result['toxicity_percentage']
        category = result['toxicity_level']
        technique = result['classification_technique']
        
        # Determinar si el resultado es correcto
        if "baja" in case['expected'].lower() and toxicity <= 30:
            status = "âœ… CORRECTO"
        elif "alta" in case['expected'].lower() and toxicity >= 60:
            status = "âœ… CORRECTO"
        elif "moderada" in case['expected'].lower() and 30 < toxicity < 60:
            status = "âœ… CORRECTO"
        else:
            status = "âš ï¸ INESPERADO"
        
        print(f"Resultado: {toxicity:.1f}% tÃ³xico ({category})")
        print(f"TÃ©cnica: {technique}")
        print(f"Tiempo: {analysis_time:.1f}ms")
        print(f"Estado: {status}")
        
        # Mostrar explicaciones si estÃ¡n disponibles
        if result.get('details', {}).get('explanations'):
            print("Explicaciones:")
            for cat, explanation in result['details']['explanations'].items():
                print(f"  â€¢ {cat}: {explanation}")
        
        results.append({
            "case": case['title'],
            "toxicity": toxicity,
            "expected": case['expected'],
            "status": status
        })
        
        print("-" * 50)
    
    # Resumen final
    print("\n" + "=" * 70)
    print("ğŸ“Š RESUMEN DE LA DEMOSTRACIÃ“N")
    print("=" * 70)
    
    correct_results = [r for r in results if "CORRECTO" in r['status']]
    unexpected_results = [r for r in results if "INESPERADO" in r['status']]
    
    print(f"âœ… Resultados correctos: {len(correct_results)}/{len(results)}")
    print(f"âš ï¸ Resultados inesperados: {len(unexpected_results)}/{len(results)}")
    
    if correct_results:
        avg_toxicity = sum(r['toxicity'] for r in correct_results) / len(correct_results)
        print(f"ğŸ“ˆ Toxicidad promedio: {avg_toxicity:.1f}%")
    
    print("\nğŸ¯ Beneficios del AnÃ¡lisis Contextual:")
    print("   â€¢ Detecta negaciones y contexto positivo")
    print("   â€¢ Evita falsos positivos en frases como 'no eres tonto'")
    print("   â€¢ AnÃ¡lisis semÃ¡ntico en lugar de solo palabras clave")
    print("   â€¢ Mayor precisiÃ³n en la clasificaciÃ³n de toxicidad")
    print("   â€¢ Explicaciones detalladas del anÃ¡lisis")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ Â¡AnÃ¡lisis contextual implementado exitosamente!")
    print("=" * 70)

if __name__ == "__main__":
    demo_contextual_analysis()
