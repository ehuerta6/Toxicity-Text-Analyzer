#!/usr/bin/env python3
"""
ğŸ§ª Test de Rendimiento ML - ToxiGuard
Prueba los modelos ML con diferentes tipos de texto
"""

import sys
import os

# Agregar el directorio del proyecto al path
sys.path.insert(0, str(os.path.dirname(__file__)))

def test_ml_models():
    """Test directo de los modelos ML"""
    print("ğŸ” Probando modelos ML directamente...")
    
    try:
        from app.ml_models import ml_classifier, hybrid_classifier
        
        print(f"âœ… ML Classifier: {ml_classifier.model_type}")
        print(f"   - Entrenado: {ml_classifier.is_trained}")
        print(f"   - Modelo: {type(ml_classifier.model).__name__}")
        
        print(f"\nâœ… Hybrid Classifier:")
        print(f"   - ML Entrenado: {hybrid_classifier.ml_classifier.is_trained}")
        print(f"   - ML Modelo: {type(hybrid_classifier.ml_classifier.model).__name__}")
        
        return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def test_toxicity_analysis():
    """Test del anÃ¡lisis de toxicidad con diferentes textos"""
    print("\nğŸ” Probando anÃ¡lisis de toxicidad...")
    
    try:
        from app.services import ToxicityClassifier
        classifier = ToxicityClassifier()
        
        # Casos de prueba
        test_cases = [
            ("Hola, Â¿cÃ³mo estÃ¡s?", "Texto normal"),
            ("Eres un idiota estÃºpido", "Insulto severo"),
            ("Este video es tonto pero divertido", "Contexto mixto"),
            ("Odio a todas las personas como tÃº", "Acoso"),
            ("Los negros son inferiores", "DiscriminaciÃ³n"),
            ("Compra esto ahora, oferta limitada", "Spam"),
            ("La comida estÃ¡ deliciosa", "Texto positivo"),
            ("Eres feo y aburrido", "Insulto leve"),
            ("Te voy a matar", "Amenaza"),
            ("El clima estÃ¡ muy agradable hoy", "Texto neutral")
        ]
        
        print("ğŸ“Š Resultados del anÃ¡lisis:")
        print("-" * 80)
        
        for text, description in test_cases:
            try:
                result = classifier.analyze_text(text)
                is_toxic, score, labels, text_length, word_count, category, toxicity_percentage = result
                
                # Determinar quÃ© clasificador se usÃ³
                classifier_used = "ML" if "ml_enhanced" in labels else "Mejorado" if "improved" in labels else "Legacy"
                
                print(f"ğŸ“ '{text[:30]}{'...' if len(text) > 30 else ''}'")
                print(f"   ğŸ“‹ DescripciÃ³n: {description}")
                print(f"   ğŸ¤– Clasificador: {classifier_used}")
                print(f"   âš ï¸ TÃ³xico: {'SÃ' if is_toxic else 'NO'}")
                print(f"   ğŸ“Š Score: {score:.3f}")
                print(f"   ğŸ¯ Porcentaje: {toxicity_percentage}%")
                print(f"   ğŸ·ï¸ CategorÃ­a: {category or 'N/A'}")
                print(f"   ğŸ·ï¸ Etiquetas: {labels}")
                print("-" * 80)
                
            except Exception as e:
                print(f"âŒ Error analizando '{text}': {e}")
                print("-" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en anÃ¡lisis: {e}")
        return False

def test_ml_prediction():
    """Test directo de predicciÃ³n ML"""
    print("\nğŸ” Probando predicciÃ³n ML directa...")
    
    try:
        from app.ml_models import ml_classifier
        
        if not ml_classifier.is_trained:
            print("âš ï¸ Modelo ML no estÃ¡ entrenado. Intentando cargar modelo guardado...")
            
            # Intentar cargar el modelo entrenado
            model_path = "models/logistic_regression_model.pkl"
            if os.path.exists(model_path):
                print(f"ğŸ“ Modelo encontrado en: {model_path}")
                # AquÃ­ podrÃ­amos implementar la carga del modelo
                print("â„¹ï¸ Para usar el modelo entrenado, necesitamos implementar la carga automÃ¡tica")
            else:
                print("âŒ No se encontrÃ³ modelo entrenado")
                return False
        
        # Test con texto simple
        test_text = "Eres un idiota estÃºpido"
        print(f"ğŸ§ª Probando texto: '{test_text}'")
        
        # Intentar predicciÃ³n
        try:
            is_toxic, prob, score = ml_classifier.predict_toxicity(test_text)
            print(f"âœ… PredicciÃ³n ML exitosa:")
            print(f"   - TÃ³xico: {is_toxic}")
            print(f"   - Probabilidad: {prob:.3f}")
            print(f"   - Score: {score:.3f}")
        except Exception as e:
            print(f"âš ï¸ PredicciÃ³n ML fallÃ³: {e}")
            print("â„¹ï¸ Esto es normal si el modelo no estÃ¡ entrenado")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en test ML: {e}")
        return False

def main():
    """FunciÃ³n principal de testing"""
    print("ğŸš€ Iniciando test de rendimiento ML...\n")
    
    tests = [
        test_ml_models,
        test_toxicity_analysis,
        test_ml_prediction
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âŒ Error ejecutando test: {e}")
    
    print(f"\nğŸ“Š Resultados: {passed}/{total} tests pasaron")
    
    if passed == total:
        print("ğŸ‰ Todos los tests pasaron! Los modelos ML estÃ¡n funcionando perfectamente.")
    else:
        print("âš ï¸ Algunos tests fallaron. Revisar errores arriba.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
