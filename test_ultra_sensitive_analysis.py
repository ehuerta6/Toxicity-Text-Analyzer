#!/usr/bin/env python3
"""
ğŸ§ª Prueba del Sistema Ultra-Sensible de AnÃ¡lisis de Toxicidad - ToxiGuard
Demuestra la capacidad del nuevo clasificador para detectar textos ofensivos con mayor precisiÃ³n
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend', 'app'))

from advanced_toxicity_classifier import advanced_toxicity_classifier

def test_ultra_sensitive_analysis():
    """Prueba el anÃ¡lisis ultra-sensible con diferentes tipos de textos"""
    
    print("ğŸš¨ PRUEBA DEL SISTEMA ULTRA-SENSIBLE DE TOXIGUARD")
    print("=" * 60)
    
    # Textos de prueba con diferentes niveles de toxicidad
    test_texts = [
        # Texto seguro
        "Hola, Â¿cÃ³mo estÃ¡s? Espero que tengas un buen dÃ­a.",
        
        # Texto con insultos leves
        "Eres un poco tonto y aburrido, pero no te preocupes.",
        
        # Texto con insultos moderados
        "Eres un idiota y estupido, no sabes nada de la vida.",
        
        # Texto con insultos severos
        "Eres un pendejo cabron, hijo de puta, eres una basura.",
        
        # Texto con amenazas
        "Te voy a matar, te odio, quiero que mueras miserablemente.",
        
        # Texto con discriminaciÃ³n
        "Los negros son inferiores, las mujeres son dÃ©biles, odio a los gays.",
        
        # Texto largo con mÃºltiples insultos
        "Eres un idiota completo, pendejo cabron, hijo de puta. No sabes nada de la vida y eres una basura humana. Te odio y quiero que mueras miserablemente. Eres un racista, xenofobo y homofobo. Eres la peor persona que he conocido.",
        
        # Texto con negaciones (deberÃ­a ser menos tÃ³xico)
        "No eres tonto, no eres idiota, no te odio, no quiero hacerte daÃ±o.",
        
        # Texto mixto (algunas palabras tÃ³xicas, otras no)
        "Hola amigo, eres un poco lento pero no te preocupes. Eres inteligente en otras cosas."
    ]
    
    print("\nğŸ“Š RESULTADOS DEL ANÃLISIS ULTRA-SENSIBLE:")
    print("-" * 60)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ” TEXTO {i}:")
        print(f"   '{text[:80]}{'...' if len(text) > 80 else ''}'")
        
        try:
            # Analizar el texto
            result = advanced_toxicity_classifier.analyze_text(text)
            
            # Mostrar resultados
            print(f"   ğŸ“ˆ Toxicidad: {result['toxicity_percentage']:.1f}%")
            print(f"   ğŸ¯ Confianza: {result['confidence']:.1f}")
            print(f"   ğŸ·ï¸  CategorÃ­a: {result['toxicity_level']}")
            print(f"   ğŸš¨ TÃ³xico: {'SÃ' if result['is_toxic'] else 'NO'}")
            
            # Mostrar categorÃ­as detectadas
            if result['details']['detected_categories']:
                print(f"   ğŸ“‹ CategorÃ­as: {', '.join(result['details']['detected_categories'])}")
            
            # Mostrar breakdown de severidad si estÃ¡ disponible
            if 'severity_breakdown' in result['details'] and result['details']['severity_breakdown']:
                print(f"   ğŸš¨ Breakdown de Severidad:")
                for category, breakdown in result['details']['severity_breakdown'].items():
                    print(f"      - {category}: {breakdown['avg_severity']:.2f} ({breakdown['match_count']} coincidencias)")
            
            # Mostrar explicaciones
            if result['details']['explanations']:
                print(f"   ğŸ’¡ Explicaciones:")
                for category, explanation in result['details']['explanations'].items():
                    print(f"      - {category}: {explanation}")
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
        
        print("-" * 40)
    
    print("\nâœ… PRUEBA COMPLETADA")
    print("\nğŸ¯ CARACTERÃSTICAS DEL SISTEMA ULTRA-SENSIBLE:")
    print("   â€¢ Umbrales ultra-sensibles para evitar valores triviales")
    print("   â€¢ PonderaciÃ³n de severidad por palabra")
    print("   â€¢ AnÃ¡lisis de repeticiÃ³n y densidad tÃ³xica")
    print("   â€¢ DetecciÃ³n de mÃºltiples categorÃ­as simultÃ¡neamente")
    print("   â€¢ Explicaciones detalladas de cada detecciÃ³n")

if __name__ == "__main__":
    test_ultra_sensitive_analysis()
